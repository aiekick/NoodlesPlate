A first texture
===============

````{tab} With webgpu.hpp
*Resulting code:* [`step060`](https://github.com/eliemichel/LearnWebGPU-Code/tree/step060)
````

````{tab} Vanilla webgpu.h
*Resulting code:* [`step060-vanilla`](https://github.com/eliemichel/LearnWebGPU-Code/tree/step060-vanilla)
````

Textures play a very important role in a rendering pipeline, both for 2D or 3D graphics.

Theoretically, a GPU buffer could be used to store the texture data, and the shader would compute at which offset within this buffer a given pixel belongs. But this would be **inefficient** (and maybe not even supported on some devices). Texture access is actually handled by dedicated fixed units of the GPU.

This is why although all types of **resources** live in the VRAM, **buffers**, **textures** and **storage textures** are different objects in the WebGPU API (as well as in all other graphics APIs).

Texture creation
----------------

Let us get back to the texture that we had to create for the depth buffer, and copy it to create a new texture:

````{tab} With webgpu.hpp
```C++
TextureDescriptor textureDesc;
// [...] setup descriptor
Texture texture = device.createTexture(textureDesc);
```
````

````{tab} Vanilla webgpu.h
```C++
WGPUTextureDescriptor textureDesc;
textureDesc.nextInChain = nullptr;
// [...] setup descriptor
WGPUTexture texture = wgpuDeviceCreateTexture(device, &textureDesc);
```
````

### Size

A simple setting is the size of the texture (which I arbitrarily set to a power of 2 because it usually helps the GPU in aligning memory):

```C++
textureDesc.dimension = TextureDimension::_2D;
textureDesc.size = { 256, 256, 1 };
//                             ^ ignored because it is a 2D texture
```

A texture can have 1, 2 or 3 dimensions. Meaning it is either a 1D color gradient, a 2D image of a 3D grid of voxels.

> ðŸ¤” When should I use a 1D texture rather than a simple buffer?

A texture allows to **sample** values **continuously**. For instance even if your 1D texture has only 10 texels (a texel is a pixel of a texture), you may sample its value at a coordinate 4.5 / 10 and you would automatically get a mix of texels 4 and 5.

Another powerful feature of textures is the possibility to sample the **average** value over a neighborhood of a texel, thanks to **mip-maps**.

these makes a texture **contain multiple images** (also called *subresources*). So the texture size is not only the `size` field but also the number of mip-maps given by `mipLevelCount`.

We'll get to why this is useful later on, for now we set the mip level count to the minimum value $1$ to deactivate this feature:

```C++
textureDesc.mipLevelCount = 1;
```

Textures are also able to store more than 1 color per texel, thanks to multisampling. This is typically used for handling anti-aliasing ([MSAA](https://en.wikipedia.org/wiki/Multisample_anti-aliasing)). Again, we do not use this for our texture:

```C++
textureDesc.sampleCount = 1;
```

Both the mip level count and the sample count have an impact on the memory size of the texture, so when not using these features, setting them to $1$ (the minimum) saves memory.

### Format

The texture format indicates at the same time the number of channels in the texture (R, RG, RGB, RGBA), their order (RGBA, BGRA, etc.), and the way each channel is encoded, including the number of bits and the scale.

For instance, `RGBA8Unorm` means 4 channels, with 8 bits per channel which represent unsigned values (U) normalized (norm) so that they are manipulated as real numbers in the range $(0,1)$ (instead of integers in $(0,255)$ for instance).

```C++
depthTextureDesc.format = TextureFormat::RGBA8Unorm;
```

### Misc

Like buffers, textures must declare their intended usage, so that they can be places in more appropriate parts of the memory by the GPU's memory allocator.

In order to be able to copy pixel data from C++, the texture needs the `CopyDst` usage. And we will then use the texture by sampling it from a shader, so it must be declared with the `TextureBinding` usage:

```C++
depthTextureDesc.usage = TextureUsage::TextureBinding | TextureUsage::CopyDst;
```

Lastly, we initialize the texture view settings to 0 (more on this later):

```C++
depthTextureDesc.viewFormatCount = 0;
depthTextureDesc.viewFormats = nullptr;
```

Uploading texture data
----------------------

### Test data

Our first texture will be a simple gradient, which we can define in our C++ code as follows:

```C++
// Create image data
std::vector<uint8_t> pixels(4 * textureDesc.size.width * textureDesc.size.height);
for (uint32_t i = 0; i < textureDesc.size.width; ++i) {
	for (uint32_t j = 0; j < textureDesc.size.height; ++j) {
		uint8_t *p = &pixels[4 * (j * textureDesc.size.width + i)];
		p[0] = (uint8_t)i; // r
		p[1] = (uint8_t)j; // g
		p[2] = 128; // b
		p[3] = 255; // a
	}
}
```

```{figure} /images/gradient-texture.png
:align: center
:class: with-shadow
The test texture that we generate in our C++ code.
```

### Write texture

Uploading this pixel data to the texture uses `Queue::writeTexture`, but a call to `Queue::writeTexture` is a bit more complicated than `Queue::writeBuffer`. Since write texture has a lot of arguments, they are grouped in substructures:

````{tab} With webgpu.hpp
```C++
// Arguments telling how the C++ side pixel memory is laid out
TextureDataLayout source;
// [...]

// Arguments telling which part of the texture we upload to
// (together with the last argument of writeTexture)
ImageCopyTexture destination;
// [...]

queue.writeTexture(destination, pixels.data(), pixels.size(), source, textureDesc.size);
```
````

````{tab} Vanilla webgpu.h
```C++
// Arguments telling which part of the texture we upload to
// (together with the last argument of writeTexture)
WGPUImageCopyTexture destination;
// [...]

// Arguments telling how the C++ side pixel memory is laid out
WGPUTextureDataLayout source;
// [...]

wgpuQueueWriteTexture(queue, destination, pixels.data(), pixels.size(), source, textureDesc.size);
```
````

### Destination

The `writeTexture` procedure writes only one image (subresource of the texture) at a time. The argument `destination.mipLevel` tells which one we target, and in our case we have only one mip level.

The `destination.origin` is the equivalent of the offset argument of `Queue::writeBuffer`. Together with the `writeSize` argument, it tells which part of the image gets updated, so that we may update only a small part of it. Finally, the aspect is not relevant for color textures.

````{tab} With webgpu.hpp
```C++
destination.texture = texture;
destination.mipLevel = 0;
destination.origin = { 0, 0, 0 }; // equivalent of the offset argument of Queue::writeBuffer
destination.aspect = TextureAspect::All; // only relevant for depth/Stencil textures
```
````

````{tab} Vanilla webgpu.h
```C++
destination.texture = texture;
destination.mipLevel = 0;
destination.origin = { 0, 0, 0 }; // equivalent of the offset argument of Queue::writeBuffer
destination.aspect = WGPUTextureAspect_All; // only relevant for depth/Stencil textures
```
````

### Source

The `source` layout tells how to read from the buffer. In our case, the data is contiguous, so we set this as follows:

```C++
source.offset = 0;
source.bytesPerRow = 4 * textureDesc.size.width;
source.rowsPerImage = textureDesc.size.height;
```

```{note}
When loading data from an image file, rows might be aligned to a round number of bytes, hence `source.bytesPerRow` would be slightly bigger than `4 * textureDesc.size.width`.
```

Texture drawing
---------------

Great, we have loaded a texture to the GPU memory... But what can we do with it? How can we check that it was correctly uploaded?

Unfortunately, there is no quick way to copy a texture to the texture view `nextTexture` returned by the swap chain. Instead, we **bind our texture to the render pipeline** and sample it from our shader.

### Binding layout

This binding is close to the uniform buffer binding. We first need to add it to the bind group **layout**. We first slightly reorganize our code to handle multiple bindings:

````{tab} With webgpu.hpp
```C++
// Create binding layouts

// Since we now have 2 bindings, we use a vector to store them
std::vector<BindGroupLayoutEntry> bindingLayoutEntries(2, Default);

// The uniform buffer binding that we already had
BindGroupLayoutEntry& bindingLayout = bindingLayoutEntries[0];
bindingLayout.binding = 0;
bindingLayout.visibility = ShaderStage::Vertex | ShaderStage::Fragment;
bindingLayout.buffer.type = BufferBindingType::Uniform;
bindingLayout.buffer.minBindingSize = sizeof(MyUniforms);

// The texture binding
BindGroupLayoutEntry& textureBindingLayout = bindingLayoutEntries[1];
// [...] Setup texture binding

// Create a bind group layout
BindGroupLayoutDescriptor bindGroupLayoutDesc{};
bindGroupLayoutDesc.entryCount = (uint32_t)bindingLayoutEntries.size();
bindGroupLayoutDesc.entries = bindingLayoutEntries.data();
BindGroupLayout bindGroupLayout = device.createBindGroupLayout(bindGroupLayoutDesc);
```
````

````{tab} Vanilla webgpu.h
```C++
// Create binding layouts

// Since we now have 2 bindings, we use a vector to store them
std::vector<WGPUBindGroupLayoutEntry> bindingLayoutEntries(2);

// The uniform buffer binding that we already had
WGPUBindGroupLayoutEntry& bindingLayout = bindingLayoutEntries[0];
setDefaults(bindingLayout);
bindingLayout.binding = 0;
bindingLayout.visibility = WGPUShaderStage_Vertex | WGPUShaderStage_Fragment;
bindingLayout.buffer.type = WGPUBufferBindingType_Uniform;
bindingLayout.buffer.minBindingSize = sizeof(MyUniforms);

// The texture binding
WGPUBindGroupLayoutEntry& textureBindingLayout = bindingLayoutEntries[1];
setDefaults(textureBindingLayout);
// [...] Setup texture binding

// Create a bind group layout
WGPUBindGroupLayoutDescriptor bindGroupLayoutDesc{};
bindGroupLayoutDesc.nextInChain = nullptr;
bindGroupLayoutDesc.entryCount = (uint32_t)bindingLayoutEntries.size();
bindGroupLayoutDesc.entries = bindingLayoutEntries.data();
WGPUBindGroupLayout bindGroupLayout = wgpuDeviceCreateBindGroupLayout(device, bindGroupLayoutDesc);
```
````

We can now specifically setup our texture binding layout:

````{tab} With webgpu.hpp
```C++
// Setup texture binding
textureBindingLayout.binding = 1;
textureBindingLayout.visibility = ShaderStage::Fragment;
textureBindingLayout.texture.sampleType = TextureSampleType::Float;
textureBindingLayout.texture.viewDimension = TextureViewDimension::_2D;
```
````

````{tab} Vanilla webgpu.h
```C++
// Setup texture binding
textureBindingLayout.binding = 1;
textureBindingLayout.visibility = WGPUShaderStage_Fragment;
textureBindingLayout.texture.sampleType = WGPUTextureSampleType_Float;
textureBindingLayout.texture.viewDimension = WGPUTextureViewDimension_2D;
```
````

The visibility is set to the fragment shader only, we will not sample this texture in the vertex shader.

The texture sample type tells which variable type will be returned in the shader code when sampling the texture. Since our texture uses a normalized format (`RGBAUnorm`), its values are represented as floats.

### Binding

````{tab} With webgpu.hpp
```C++
// Create a binding
std::vector<BindGroupEntry> bindings(2);

bindings[0].binding = 0;
bindings[0].buffer = uniformBuffer;
bindings[0].offset = 0;
bindings[0].size = sizeof(MyUniforms);

bindings[1].binding = 1;
bindings[1].textureView = ???;

BindGroupDescriptor bindGroupDesc{};
bindGroupDesc.layout = bindGroupLayout;
bindGroupDesc.entryCount = (uint32_t)bindings.size();
bindGroupDesc.entries = bindings.data();
BindGroup bindGroup = device.createBindGroup(bindGroupDesc);
```
````

````{tab} Vanilla webgpu.h
```C++
// Create a binding
std::vector<WGPUBindGroupEntry> bindings(2);

bindings[0].binding = 0;
bindings[0].buffer = uniformBuffer;
bindings[0].offset = 0;
bindings[0].size = sizeof(MyUniforms);

bindings[1].binding = 1;
bindings[1].textureView = ???;

WGPUBindGroupDescriptor bindGroupDesc{};
bindGroupDesc.nextInChain = nullptr;
bindGroupDesc.layout = bindGroupLayout;
bindGroupDesc.entryCount = (uint32_t)bindings.size();
bindGroupDesc.entries = bindings.data();
WGPUBindGroup bindGroup = wgpuDeviceCreateBindGroup(device, bindGroupDesc);
```
````

We create a texture view from our texture.

````{tab} With webgpu.hpp
```C++
TextureViewDescriptor textureViewDesc;
textureViewDesc.aspect = TextureAspect::All;
textureViewDesc.baseArrayLayer = 0;
textureViewDesc.arrayLayerCount = 1;
textureViewDesc.baseMipLevel = 0;
textureViewDesc.mipLevelCount = 1;
textureViewDesc.dimension = TextureViewDimension::_2D;
textureViewDesc.format = textureDesc.format;
TextureView textureView = texture.createView(textureViewDesc);
```
````

````{tab} Vanilla webgpu.h
```C++
TextureViewDescriptor textureViewDesc;
textureViewDesc.aspect = WGPUTextureAspect_All;
textureViewDesc.baseArrayLayer = 0;
textureViewDesc.arrayLayerCount = 1;
textureViewDesc.baseMipLevel = 0;
textureViewDesc.mipLevelCount = 1;
textureViewDesc.dimension = WGPUTextureViewDimension_2D;
textureViewDesc.format = textureDesc.format;
TextureView textureView = texture.createView(textureViewDesc);
```
````

### Shader

We can now declare a global variable of type `texture_2d<f32>` in our shader, attached to binding 1 in bind group 0. The function `textureLoad` returns the raw texel value at a given coordinate:

```rust
@group(0) @binding(1) var gradientTexture: texture_2d<f32>;

fn fs_main(in: VertexOutput) -> @location(0) vec4<f32> {
	let color = textureLoad(gradientTexture, vec2<i32>(in.position.xy), 0).rgb;
	// ...
}
```

In order to better see the result, we make sure that there is a fragment for each pixel by drawing a plane that covers the whole screen. Load the file [plane.obj](../../data/plane.obj) and set your matrices to:

```C++
bool success = loadGeometryFromObj(RESOURCE_DIR "/plane.obj", vertexData);

// [...]

uniforms.modelMatrix = mat4x4(1.0);
uniforms.viewMatrix = glm::scale(mat4x4(1.0), vec3(1.0f));
uniforms.projectionMatrix = glm::ortho(-1, 1, -1, 1, -1, 1);
```

```{figure} /images/gradient-texture-window.png
:align: center
:class: with-shadow
Our first texture displayed by drawing a full-screen quad.
```

Feel free to play with other formulas to create the texture data:

```C++
// Create image data
std::vector<uint8_t> pixels(4 * textureDesc.size.width * textureDesc.size.height);
for (uint32_t i = 0; i < textureDesc.size.width; ++i) {
	for (uint32_t j = 0; j < textureDesc.size.height; ++j) {
		uint8_t *p = &pixels[4 * (j * textureDesc.size.width + i)];
		p[0] = (i / 16) % 2 == (j / 16) % 2 ? 255 : 0; // r
		p[1] = ((i - j) / 16) % 2 == 0 ? 255 : 0; // g
		p[2] = ((i + j) / 16) % 2 == 0 ? 255 : 0; // b
		p[3] = 255; // a
	}
}
```

```{figure} /images/other-texture-window.png
:align: center
:class: with-shadow
Changing the pixel array indeed changes the displayed image.
```

Conclusion
----------

We have seen how to create a texture and how to access its pixels from a shader. In the next chapter we will see how to map a texture onto a 3D mesh. And we will realize that we miss a very important ingredient: to fully benefit from the power of a texture, it must be accessed through a **sampler**.

````{tab} With webgpu.hpp
*Resulting code:* [`step060`](https://github.com/eliemichel/LearnWebGPU-Code/tree/step060)
````

````{tab} Vanilla webgpu.h
*Resulting code:* [`step060-vanilla`](https://github.com/eliemichel/LearnWebGPU-Code/tree/step060-vanilla)
````
